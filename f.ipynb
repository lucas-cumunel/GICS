{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad96f88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 girl names ✅\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "async def scrape_page(page, url):\n",
    "    await page.goto(url, timeout=0)\n",
    "    try:\n",
    "        await page.wait_for_selector(\"ul.name-list li h3\", timeout=10000)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "    # Scroll to load dynamic content\n",
    "    previous_height = 0\n",
    "    for _ in range(10):  # scroll several times\n",
    "        current_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "        if current_height == previous_height:\n",
    "            break\n",
    "        previous_height = current_height\n",
    "        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        await page.wait_for_timeout(1000)\n",
    "\n",
    "    # ✅ Make sure elements have loaded\n",
    "    elements = await page.query_selector_all(\"ul.name-list li h3\")\n",
    "    names = []\n",
    "    for el in elements:\n",
    "        text = (await el.inner_text()).strip()\n",
    "        if text:\n",
    "            names.append(text)\n",
    "    return names\n",
    "\n",
    "\n",
    "\n",
    "async def scrape_letter(playwright, gender, letter):\n",
    "    browser = await playwright.chromium.launch(headless=True)\n",
    "    context = await browser.new_context(\n",
    "        user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                   \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                   \"Chrome/118.0.5993.118 Safari/537.36\"\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    page_number = 1\n",
    "    while True:\n",
    "        # Build URL\n",
    "        url = f\"https://japanese-names.info/first-names/gender/{gender}-name/start-with-{letter}-{gender}/page/{page_number}\"\n",
    "\n",
    "        try:\n",
    "            # ✅ Open a *new tab* for each page\n",
    "            page = await context.new_page()\n",
    "            names = await scrape_page(page, url)\n",
    "            await page.close()  # ✅ Close tab to free memory\n",
    "\n",
    "            if not names:\n",
    "                break\n",
    "            results.extend(names)\n",
    "            page_number += 1\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error on {url}: {e}\")\n",
    "            break  # stop if page doesn't exist or any error occurs\n",
    "\n",
    "    await browser.close()\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "async def scrape_all(gender):\n",
    "    async with async_playwright() as playwright:\n",
    "        tasks = [scrape_letter(playwright, gender, letter) for letter in string.ascii_lowercase]\n",
    "        all_names = await asyncio.gather(*tasks)\n",
    "\n",
    "    flat_list = [name for sublist in all_names for name in sublist]\n",
    "    df = pd.DataFrame(flat_list, columns=[f\"{gender}_names\"])\n",
    "    return df\n",
    "\n",
    "async def main():\n",
    "    girls_task = scrape_all(\"girl\")\n",
    "    #boys_task = scrape_all(\"boy\")\n",
    "    #girls_df, boys_df = await asyncio.gather(girls_task, boys_task)\n",
    "    [girls_df] = await asyncio.gather(girls_task)\n",
    "\n",
    "    print(f\"Found {len(girls_df)} girl names ✅\")\n",
    "    #print(f\"Found {len(boys_df)} boy names ✅\")\n",
    "\n",
    "    #girls_df.to_csv(\"japanese_girls.csv\", index=False)\n",
    "    #boys_df.to_csv(\"japanese_boys.csv\", index=False)\n",
    "\n",
    "# For Jupyter\n",
    "girls_df = await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e3fb2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60 names ✅\n",
      "Akari\n",
      "Ayaka\n",
      "Arisa\n",
      "Asana\n",
      "Akina\n",
      "Aki\n",
      "Aika\n",
      "Airi\n",
      "Ayami\n",
      "Akiri\n",
      "Aoi\n",
      "Ayana\n",
      "Asuka\n",
      "Ayuka\n",
      "Ami\n",
      "Akiho\n",
      "Ayumi\n",
      "Asaka\n",
      "Akie\n",
      "Akiko\n",
      "Akane\n",
      "Ayu\n",
      "Aina\n",
      "Arisu\n",
      "Asami\n",
      "Akika\n",
      "Amika\n",
      "Ayame\n",
      "Ayuna\n",
      "Aya\n",
      "Aria\n",
      "Ai\n",
      "Arika\n",
      "Amiri\n",
      "Aimi\n",
      "Ako\n",
      "Akira\n",
      "Akihi\n",
      "Ayane\n",
      "Asahi\n",
      "Ayako\n",
      "Asune\n",
      "Anna\n",
      "Arina\n",
      "Asako\n",
      "Ayano\n",
      "Asa\n",
      "Asuna\n",
      "Akana\n",
      "Ayuri\n",
      "Aiko\n",
      "Akiha\n",
      "Ayae\n",
      "Ayari\n",
      "Azusa\n",
      "Akino\n",
      "Azumi\n",
      "Amina\n",
      "Akiyo\n",
      "Aisa\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "URL = \"https://japanese-names.info/first-names/gender/girl-name/start-with-a-girl\"\n",
    "\n",
    "async def scrape_names(url):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        context = await browser.new_context(\n",
    "            user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                       \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                       \"Chrome/118.0.5993.118 Safari/537.36\"\n",
    "        )\n",
    "        page = await context.new_page()\n",
    "        await page.goto(url, timeout=0)\n",
    "\n",
    "        # Wait for at least one name to appear\n",
    "        await page.wait_for_selector(\"ul.name-list li h3\", timeout=10000)\n",
    "\n",
    "        # Scroll to load all names\n",
    "        previous_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "        while True:\n",
    "            await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            await page.wait_for_timeout(1500)  # wait for new content to load\n",
    "            current_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "            if current_height == previous_height:\n",
    "                break\n",
    "            previous_height = current_height\n",
    "\n",
    "        # Extract names with proper inner_text evaluation\n",
    "        name_elements = await page.query_selector_all(\"ul.name-list li h3\")\n",
    "        names = []\n",
    "        for el in name_elements:\n",
    "            # Use evaluate to get fully rendered text\n",
    "            text = await el.evaluate(\"(node) => node.textContent.trim()\")\n",
    "            if text:\n",
    "                names.append(text)\n",
    "\n",
    "        await browser.close()\n",
    "        return names\n",
    "\n",
    "async def main():\n",
    "    names = await scrape_names(URL)\n",
    "    print(f\"Found {len(names)} names ✅\")\n",
    "    for name in names:\n",
    "        print(name)\n",
    "\n",
    "# In Jupyter\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4ee7426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping girl names...\n",
      "Found 748 girl names ✅\n",
      "Scraping boy names...\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import string\n",
    "import pandas as pd\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "BASE_URL = \"https://japanese-names.info/first-names/gender/{gender}-name/start-with-{letter}\"\n",
    "GIRL = \"girl\"\n",
    "BOY = \"boy\"\n",
    "\n",
    "async def scrape_page(page, url):\n",
    "    \"\"\"Scrape one page and return list of names\"\"\"\n",
    "    await page.goto(url, timeout=0)\n",
    "    # Wait for names to load\n",
    "    try:\n",
    "        await page.wait_for_selector(\"ul.name-list li h3\", timeout=5000)\n",
    "    except:\n",
    "        return []  # no names on this page\n",
    "\n",
    "    # Scroll to ensure dynamic content loads\n",
    "    previous_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "    while True:\n",
    "        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        await page.wait_for_timeout(1000)\n",
    "        current_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "        if current_height == previous_height:\n",
    "            break\n",
    "        previous_height = current_height\n",
    "\n",
    "    # Extract names\n",
    "    name_elements = await page.query_selector_all(\"ul.name-list li h3\")\n",
    "    names = [await el.evaluate(\"(node) => node.textContent.trim()\") for el in name_elements if await el.evaluate(\"(node) => node.textContent.trim()\")]\n",
    "    return names\n",
    "\n",
    "async def scrape_gender(gender):\n",
    "    all_data = []\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        context = await browser.new_context(\n",
    "            user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                       \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                       \"Chrome/118.0.5993.118 Safari/537.36\"\n",
    "        )\n",
    "        page = await context.new_page()\n",
    "\n",
    "        for letter in [\"a\"] :#string.ascii_lowercase:\n",
    "            page_num = 1\n",
    "            while True:\n",
    "                if gender == GIRL:\n",
    "                    url = f\"{BASE_URL.format(gender=gender, letter=letter)}/page/{page_num}/\"\n",
    "                else:  # boys have no page number on single-page\n",
    "                    url = f\"{BASE_URL.format(gender=gender, letter=letter)}\" if page_num == 1 else f\"{BASE_URL.format(gender=gender, letter=letter)}/page/{page_num}/\"\n",
    "\n",
    "                names = await scrape_page(page, url)\n",
    "                if not names:\n",
    "                    break  # stop if page has no names\n",
    "\n",
    "                # Append data\n",
    "                for name in names:\n",
    "                    all_data.append({\"letter\": letter, \"page\": page_num, \"name\": name})\n",
    "\n",
    "                page_num += 1\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "async def main():\n",
    "    print(\"Scraping girl names...\")\n",
    "    girls_df = await scrape_gender(GIRL)\n",
    "    print(f\"Found {len(girls_df)} girl names ✅\")\n",
    "\n",
    "    print(\"Scraping boy names...\")\n",
    "    #boys_df = await scrape_gender(BOY)\n",
    "    #print(f\"Found {len(boys_df)} boy names ✅\")\n",
    "\n",
    "    # Save to CSV if needed\n",
    "    #girls_df.to_csv(\"japanese_girl_names.csv\", index=False)\n",
    "    #boys_df.to_csv(\"japanese_boy_names.csv\", index=False)\n",
    "\n",
    "    return girls_df#, boys_df\n",
    "\n",
    "# In Jupyter, use:\n",
    "girls_df = await main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
