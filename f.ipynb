{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad96f88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import s3fs\n",
    "import re\n",
    "import pandas as pd\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = '1J9SZCURG0IZM0VVX37D'\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = '9CKCagFEDjzIKptrZy1sBOI2+C8+94ojH+LONi54'\n",
    "os.environ[\"AWS_SESSION_TOKEN\"] = 'eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3NLZXkiOiIxSjlTWkNVUkcwSVpNMFZWWDM3RCIsImFsbG93ZWQtb3JpZ2lucyI6WyIqIl0sImF1ZCI6WyJtaW5pby1kYXRhbm9kZSIsIm9ueXhpYSIsImFjY291bnQiXSwiYXV0aF90aW1lIjoxNzYxNjM5MDM3LCJhenAiOiJvbnl4aWEiLCJlbWFpbCI6Imx1Y2FzLmN1bXVuZWxAZW5zYWUuZnIiLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwiZXhwIjoxNzYyMjQzODYwLCJmYW1pbHlfbmFtZSI6IkN1bXVuZWwiLCJnaXZlbl9uYW1lIjoiTHVjYXMiLCJncm91cHMiOlsiVVNFUl9PTllYSUEiLCJzdGF0YXBwLXNlZ21lZGljIl0sImlhdCI6MTc2MTYzOTA2MCwiaXNzIjoiaHR0cHM6Ly9hdXRoLmxhYi5zc3BjbG91ZC5mci9hdXRoL3JlYWxtcy9zc3BjbG91ZCIsImp0aSI6Im9ucnRydDo4NzNjZTViNy04ZDQ3LWQ0NGItNDk3NC04NWNjNDA3NWJkZWUiLCJuYW1lIjoiTHVjYXMgQ3VtdW5lbCIsInBvbGljeSI6InN0c29ubHkiLCJwcmVmZXJyZWRfdXNlcm5hbWUiOiJsYWIiLCJyZWFsbV9hY2Nlc3MiOnsicm9sZXMiOlsib2ZmbGluZV9hY2Nlc3MiLCJ1bWFfYXV0aG9yaXphdGlvbiIsImRlZmF1bHQtcm9sZXMtc3NwY2xvdWQiXX0sInJlc291cmNlX2FjY2VzcyI6eyJhY2NvdW50Ijp7InJvbGVzIjpbIm1hbmFnZS1hY2NvdW50IiwibWFuYWdlLWFjY291bnQtbGlua3MiLCJ2aWV3LXByb2ZpbGUiXX19LCJyb2xlcyI6WyJvZmZsaW5lX2FjY2VzcyIsInVtYV9hdXRob3JpemF0aW9uIiwiZGVmYXVsdC1yb2xlcy1zc3BjbG91ZCJdLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGdyb3VwcyBlbWFpbCIsInNpZCI6IjdlMzg4NmFiLWUxYWEtNDZiNS04MGE2LTRkNDRiYzk0NzJkZiIsInN1YiI6ImUyZDc4NjRjLTcwMzItNDI0ZC04OTA2LWU0ZjhiNDFjYzAwMyIsInR5cCI6IkJlYXJlciJ9.oBZKGdOLRwmoT9SUMM2H5RfIPMoP22e8cotzbdlOdtscV3mC6HL1vp72nuBvp6kMECQOlsOeqVY7rSfNnMm8Jw'\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = 'us-east-1'\n",
    "fs = s3fs.S3FileSystem(\n",
    "    client_kwargs={'endpoint_url': 'https://'+'minio.lab.sspcloud.fr'},\n",
    "    key = os.environ[\"AWS_ACCESS_KEY_ID\"], \n",
    "    secret = os.environ[\"AWS_SECRET_ACCESS_KEY\"], \n",
    "    token = os.environ[\"AWS_SESSION_TOKEN\"])\n",
    "\n",
    "\n",
    "\n",
    "us_list = fs.glob(\"s3://lab/yob????.txt\")\n",
    "df=pd.DataFrame()\n",
    "for i in us_list :\n",
    "    with fs.open(f\"s3://{i}\") as f:\n",
    "        temp = pd.read_csv(f, header=None, names=['Name', 'Gender', 'Count'])\n",
    "    df=pd.concat([df,temp], axis=0,ignore_index=True)\n",
    "\n",
    "s3_path = \"s3://lab/usgnd.csv\"\n",
    "\n",
    "df.to_csv(s3_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb008c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68e4d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path = \"s3://lab/usgnd.csv\"\n",
    "\n",
    "df.to_csv(s3_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e3fb2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60 names ✅\n",
      "Akari\n",
      "Ayaka\n",
      "Arisa\n",
      "Asana\n",
      "Akina\n",
      "Aki\n",
      "Aika\n",
      "Airi\n",
      "Ayami\n",
      "Akiri\n",
      "Aoi\n",
      "Ayana\n",
      "Asuka\n",
      "Ayuka\n",
      "Ami\n",
      "Akiho\n",
      "Ayumi\n",
      "Asaka\n",
      "Akie\n",
      "Akiko\n",
      "Akane\n",
      "Ayu\n",
      "Aina\n",
      "Arisu\n",
      "Asami\n",
      "Akika\n",
      "Amika\n",
      "Ayame\n",
      "Ayuna\n",
      "Aya\n",
      "Aria\n",
      "Ai\n",
      "Arika\n",
      "Amiri\n",
      "Aimi\n",
      "Ako\n",
      "Akira\n",
      "Akihi\n",
      "Ayane\n",
      "Asahi\n",
      "Ayako\n",
      "Asune\n",
      "Anna\n",
      "Arina\n",
      "Asako\n",
      "Ayano\n",
      "Asa\n",
      "Asuna\n",
      "Akana\n",
      "Ayuri\n",
      "Aiko\n",
      "Akiha\n",
      "Ayae\n",
      "Ayari\n",
      "Azusa\n",
      "Akino\n",
      "Azumi\n",
      "Amina\n",
      "Akiyo\n",
      "Aisa\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "URL = \"https://japanese-names.info/first-names/gender/girl-name/start-with-a-girl\"\n",
    "\n",
    "async def scrape_names(url):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        context = await browser.new_context(\n",
    "            user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                       \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                       \"Chrome/118.0.5993.118 Safari/537.36\"\n",
    "        )\n",
    "        page = await context.new_page()\n",
    "        await page.goto(url, timeout=0)\n",
    "\n",
    "        # Wait for at least one name to appear\n",
    "        await page.wait_for_selector(\"ul.name-list li h3\", timeout=10000)\n",
    "\n",
    "        # Scroll to load all names\n",
    "        previous_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "        while True:\n",
    "            await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            await page.wait_for_timeout(1500)  # wait for new content to load\n",
    "            current_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "            if current_height == previous_height:\n",
    "                break\n",
    "            previous_height = current_height\n",
    "\n",
    "        # Extract names with proper inner_text evaluation\n",
    "        name_elements = await page.query_selector_all(\"ul.name-list li h3\")\n",
    "        names = []\n",
    "        for el in name_elements:\n",
    "            # Use evaluate to get fully rendered text\n",
    "            text = await el.evaluate(\"(node) => node.textContent.trim()\")\n",
    "            if text:\n",
    "                names.append(text)\n",
    "\n",
    "        await browser.close()\n",
    "        return names\n",
    "\n",
    "async def main():\n",
    "    names = await scrape_names(URL)\n",
    "    print(f\"Found {len(names)} names ✅\")\n",
    "    for name in names:\n",
    "        print(name)\n",
    "\n",
    "# In Jupyter\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4ee7426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping girl names...\n",
      "Found 748 girl names ✅\n",
      "Scraping boy names...\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import string\n",
    "import pandas as pd\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "BASE_URL = \"https://japanese-names.info/first-names/gender/{gender}-name/start-with-{letter}\"\n",
    "GIRL = \"girl\"\n",
    "BOY = \"boy\"\n",
    "\n",
    "async def scrape_page(page, url):\n",
    "    \"\"\"Scrape one page and return list of names\"\"\"\n",
    "    await page.goto(url, timeout=0)\n",
    "    # Wait for names to load\n",
    "    try:\n",
    "        await page.wait_for_selector(\"ul.name-list li h3\", timeout=5000)\n",
    "    except:\n",
    "        return []  # no names on this page\n",
    "\n",
    "    # Scroll to ensure dynamic content loads\n",
    "    previous_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "    while True:\n",
    "        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        await page.wait_for_timeout(1000)\n",
    "        current_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "        if current_height == previous_height:\n",
    "            break\n",
    "        previous_height = current_height\n",
    "\n",
    "    # Extract names\n",
    "    name_elements = await page.query_selector_all(\"ul.name-list li h3\")\n",
    "    names = [await el.evaluate(\"(node) => node.textContent.trim()\") for el in name_elements if await el.evaluate(\"(node) => node.textContent.trim()\")]\n",
    "    return names\n",
    "\n",
    "async def scrape_gender(gender):\n",
    "    all_data = []\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        context = await browser.new_context(\n",
    "            user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                       \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                       \"Chrome/118.0.5993.118 Safari/537.36\"\n",
    "        )\n",
    "        page = await context.new_page()\n",
    "\n",
    "        for letter in [\"a\"] :#string.ascii_lowercase:\n",
    "            page_num = 1\n",
    "            while True:\n",
    "                if gender == GIRL:\n",
    "                    url = f\"{BASE_URL.format(gender=gender, letter=letter)}/page/{page_num}/\"\n",
    "                else:  # boys have no page number on single-page\n",
    "                    url = f\"{BASE_URL.format(gender=gender, letter=letter)}\" if page_num == 1 else f\"{BASE_URL.format(gender=gender, letter=letter)}/page/{page_num}/\"\n",
    "\n",
    "                names = await scrape_page(page, url)\n",
    "                if not names:\n",
    "                    break  # stop if page has no names\n",
    "\n",
    "                # Append data\n",
    "                for name in names:\n",
    "                    all_data.append({\"letter\": letter, \"page\": page_num, \"name\": name})\n",
    "\n",
    "                page_num += 1\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "async def main():\n",
    "    print(\"Scraping girl names...\")\n",
    "    girls_df = await scrape_gender(GIRL)\n",
    "    print(f\"Found {len(girls_df)} girl names ✅\")\n",
    "\n",
    "    print(\"Scraping boy names...\")\n",
    "    #boys_df = await scrape_gender(BOY)\n",
    "    #print(f\"Found {len(boys_df)} boy names ✅\")\n",
    "\n",
    "    # Save to CSV if needed\n",
    "    #girls_df.to_csv(\"japanese_girl_names.csv\", index=False)\n",
    "    #boys_df.to_csv(\"japanese_boy_names.csv\", index=False)\n",
    "\n",
    "    return girls_df#, boys_df\n",
    "\n",
    "# In Jupyter, use:\n",
    "girls_df = await main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
