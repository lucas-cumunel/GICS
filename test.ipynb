{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2bfca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import s3fs\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = 'BFMNK1LUPXQ061C6PZ55'\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = '1lUWhLj95YhcD8P5t6GEMXxwlf0jyzmqrGPMURQE'\n",
    "os.environ[\"AWS_SESSION_TOKEN\"] = 'eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3NLZXkiOiJCRk1OSzFMVVBYUTA2MUM2UFo1NSIsImFsbG93ZWQtb3JpZ2lucyI6WyIqIl0sImF1ZCI6WyJtaW5pby1kYXRhbm9kZSIsIm9ueXhpYSIsImFjY291bnQiXSwiYXV0aF90aW1lIjoxNzU5MDQ2Njc3LCJhenAiOiJvbnl4aWEiLCJlbWFpbCI6Imx1Y2FzLmN1bXVuZWxAZW5zYWUuZnIiLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwiZXhwIjoxNzU5NjUxNTE1LCJmYW1pbHlfbmFtZSI6IkN1bXVuZWwiLCJnaXZlbl9uYW1lIjoiTHVjYXMiLCJncm91cHMiOlsiVVNFUl9PTllYSUEiLCJzdGF0YXBwLXNlZ21lZGljIl0sImlhdCI6MTc1OTA0NjcxNSwiaXNzIjoiaHR0cHM6Ly9hdXRoLmxhYi5zc3BjbG91ZC5mci9hdXRoL3JlYWxtcy9zc3BjbG91ZCIsImp0aSI6Im9ucnRydDplNzkxZWViNS1kYWQyLWIxZjYtMzJjZi05YTM2ODVjNTNlNGIiLCJuYW1lIjoiTHVjYXMgQ3VtdW5lbCIsInBvbGljeSI6InN0c29ubHkiLCJwcmVmZXJyZWRfdXNlcm5hbWUiOiJsYWIiLCJyZWFsbV9hY2Nlc3MiOnsicm9sZXMiOlsib2ZmbGluZV9hY2Nlc3MiLCJ1bWFfYXV0aG9yaXphdGlvbiIsImRlZmF1bHQtcm9sZXMtc3NwY2xvdWQiXX0sInJlc291cmNlX2FjY2VzcyI6eyJhY2NvdW50Ijp7InJvbGVzIjpbIm1hbmFnZS1hY2NvdW50IiwibWFuYWdlLWFjY291bnQtbGlua3MiLCJ2aWV3LXByb2ZpbGUiXX19LCJyb2xlcyI6WyJvZmZsaW5lX2FjY2VzcyIsInVtYV9hdXRob3JpemF0aW9uIiwiZGVmYXVsdC1yb2xlcy1zc3BjbG91ZCJdLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGdyb3VwcyBlbWFpbCIsInNpZCI6ImFmY2VhMjNmLTM3MGUtNDhjNC1hNGQ5LWIzZDJjMDJkYmNkOCIsInN1YiI6ImUyZDc4NjRjLTcwMzItNDI0ZC04OTA2LWU0ZjhiNDFjYzAwMyIsInR5cCI6IkJlYXJlciJ9.JF4l6bNSZ2jJniDyrrDJRipM1GFJh5a05vfMLSwOcL5vXJbg6BjsZ639IP1NSk2phHnYPPnpD8xnCk6Pub1mEw'\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = 'us-east-1'\n",
    "fs = s3fs.S3FileSystem(\n",
    "    client_kwargs={'endpoint_url': 'https://'+'minio.lab.sspcloud.fr'},\n",
    "    key = os.environ[\"AWS_ACCESS_KEY_ID\"], \n",
    "    secret = os.environ[\"AWS_SECRET_ACCESS_KEY\"], \n",
    "    token = os.environ[\"AWS_SESSION_TOKEN\"])\n",
    "\n",
    "path = \"lab/dblp (1).xml.gz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e469ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install lxml\n",
    "import gzip\n",
    "import xml.etree.ElementTree as ET\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "articles = []\n",
    "\n",
    "with fs.open(path, 'rb') as f:\n",
    "    with gzip.GzipFile(fileobj=f) as gz:\n",
    "        #parser = etree.XMLParser(recover=True, resolve_entities=True)\n",
    "        context = etree.iterparse(gz, events=(\"end\",), tag=\"article\", recover=True)\n",
    "        \n",
    "        for event, elem in context:\n",
    "            title = elem.findtext(\"title\")\n",
    "            journal = elem.findtext(\"journal\")\n",
    "            pub_date = elem.findtext(\"pub_date\")\n",
    "\n",
    "            authors = [a.text for a in elem.findall(\".//author\")]\n",
    "            authors_str = \"; \".join([a for a in authors if a])\n",
    "\n",
    "            articles.append({\n",
    "                \"title\": title,\n",
    "                \"journal\": journal,\n",
    "                \"pub_date\": pub_date,\n",
    "                \"authors\": authors_str\n",
    "            })\n",
    "\n",
    "            elem.clear()\n",
    "\n",
    "df = pd.DataFrame(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63b4a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "n_chunks = 50\n",
    "chunk_size = len(df) // n_chunks\n",
    "\n",
    "for i in range(n_chunks):\n",
    "    start = i * chunk_size\n",
    "    end = (i + 1) * chunk_size if i < n_chunks - 1 else len(df)  # last chunk includes remainder\n",
    "    \n",
    "    chunk = df.iloc[start:end].copy()\n",
    "    \n",
    "    # Split authors column into multiple columns\n",
    "    #authors_df = chunk['authors'].str.split(';', expand=True)\n",
    "    #authors_df.columns = [f'author{i+1}' for i in range(authors_df.shape[1])]\n",
    "    \n",
    "    # Combine with original chunk\n",
    "    #chunk = pd.concat([chunk.drop(columns=['authors']), authors_df], axis=1)\n",
    "    \n",
    "    chunk.to_parquet(f\"t/articles_chunk_{i}.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a780155",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "\n",
    "# Dictionary to collect coauthors per ego\n",
    "ego_dict = defaultdict(set)\n",
    "\n",
    "# List of all your chunk files\n",
    "chunk_files = glob.glob(\"t/articles_chunk_*.parquet\")\n",
    "\n",
    "for file in chunk_files:\n",
    "    df_chunk = pd.read_parquet(file)\n",
    "    \n",
    "    for authors_str in df_chunk['authors']:\n",
    "        authors = authors_str.split(';')\n",
    "        for ego in authors:\n",
    "            coauthors = set(authors) - {ego}\n",
    "            ego_dict[ego].update(coauthors)\n",
    "    \n",
    "    # Free memory after each chunk\n",
    "    del df_chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f903d5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_df = pd.DataFrame([\n",
    "    {'ego': ego, 'coauthors': \",\".join(sorted(list(coauthors)))}\n",
    "    for ego, coauthors in ego_dict.items()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810db5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_df.to_parquet(\"ego_coauthors.parquet\", index=False)\n",
    "s3_path = \"s3://lab/ego_coauthors.parquet\"  # bucket + key\n",
    "\n",
    "# Save directly to MinIO\n",
    "ego_df.to_parquet(s3_path, engine=\"pyarrow\", index=False, filesystem=fs)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
