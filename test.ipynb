{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af2bfca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml\n",
      "  Downloading lxml-6.0.2-cp313-cp313-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (3.6 kB)\n",
      "Downloading lxml-6.0.2-cp313-cp313-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lxml\n",
      "Successfully installed lxml-6.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install lxml\n",
    "import os\n",
    "import s3fs\n",
    "import gzip\n",
    "import xml.etree.ElementTree as ET\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = 'BDUCSQ8RE22Q2LMTUY75'\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = 'Y7jUkgRo+sBR2Ctfzr+djyg3a71kTOd0pSgynrYd'\n",
    "os.environ[\"AWS_SESSION_TOKEN\"] = 'eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3NLZXkiOiJCRFVDU1E4UkUyMlEyTE1UVVk3NSIsImFsbG93ZWQtb3JpZ2lucyI6WyIqIl0sImF1ZCI6WyJtaW5pby1kYXRhbm9kZSIsIm9ueXhpYSIsImFjY291bnQiXSwiYXV0aF90aW1lIjoxNzU5MDQ2Njc3LCJhenAiOiJvbnl4aWEiLCJlbWFpbCI6Imx1Y2FzLmN1bXVuZWxAZW5zYWUuZnIiLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwiZXhwIjoxNzYwNTI3MTYzLCJmYW1pbHlfbmFtZSI6IkN1bXVuZWwiLCJnaXZlbl9uYW1lIjoiTHVjYXMiLCJncm91cHMiOlsiVVNFUl9PTllYSUEiLCJzdGF0YXBwLXNlZ21lZGljIl0sImlhdCI6MTc1OTkyMjM2MywiaXNzIjoiaHR0cHM6Ly9hdXRoLmxhYi5zc3BjbG91ZC5mci9hdXRoL3JlYWxtcy9zc3BjbG91ZCIsImp0aSI6Im9ucnRydDpjODJkYzBkYS03MGM3LTI4ZGUtYjIzMS1jNWQ3MTdlNzRjMmEiLCJuYW1lIjoiTHVjYXMgQ3VtdW5lbCIsInBvbGljeSI6InN0c29ubHkiLCJwcmVmZXJyZWRfdXNlcm5hbWUiOiJsYWIiLCJyZWFsbV9hY2Nlc3MiOnsicm9sZXMiOlsib2ZmbGluZV9hY2Nlc3MiLCJ1bWFfYXV0aG9yaXphdGlvbiIsImRlZmF1bHQtcm9sZXMtc3NwY2xvdWQiXX0sInJlc291cmNlX2FjY2VzcyI6eyJhY2NvdW50Ijp7InJvbGVzIjpbIm1hbmFnZS1hY2NvdW50IiwibWFuYWdlLWFjY291bnQtbGlua3MiLCJ2aWV3LXByb2ZpbGUiXX19LCJyb2xlcyI6WyJvZmZsaW5lX2FjY2VzcyIsInVtYV9hdXRob3JpemF0aW9uIiwiZGVmYXVsdC1yb2xlcy1zc3BjbG91ZCJdLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGdyb3VwcyBlbWFpbCIsInNpZCI6ImFmY2VhMjNmLTM3MGUtNDhjNC1hNGQ5LWIzZDJjMDJkYmNkOCIsInN1YiI6ImUyZDc4NjRjLTcwMzItNDI0ZC04OTA2LWU0ZjhiNDFjYzAwMyIsInR5cCI6IkJlYXJlciJ9.hlce_P59hQ6ibPoCT7wojM-AnVPacHINBdQMTzpWMfcEVPN3MYd80TYaz7Z3gkTjxfKjhueg2WvYiC0-0JN1bQ'\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = 'us-east-1'\n",
    "fs = s3fs.S3FileSystem(\n",
    "    client_kwargs={'endpoint_url': 'https://'+'minio.lab.sspcloud.fr'},\n",
    "    key = os.environ[\"AWS_ACCESS_KEY_ID\"], \n",
    "    secret = os.environ[\"AWS_SECRET_ACCESS_KEY\"], \n",
    "    token = os.environ[\"AWS_SESSION_TOKEN\"])\n",
    "\n",
    "path = \"lab/dblp (1).xml.gz\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f041b29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60 names ✅\n",
      "Akari\n",
      "Ayaka\n",
      "Arisa\n",
      "Asana\n",
      "Akina\n",
      "Aki\n",
      "Aika\n",
      "Airi\n",
      "Ayami\n",
      "Akiri\n",
      "Aoi\n",
      "Ayana\n",
      "Asuka\n",
      "Ayuka\n",
      "Ami\n",
      "Akiho\n",
      "Ayumi\n",
      "Asaka\n",
      "Akie\n",
      "Akiko\n",
      "Akane\n",
      "Ayu\n",
      "Aina\n",
      "Arisu\n",
      "Asami\n",
      "Akika\n",
      "Amika\n",
      "Ayame\n",
      "Ayuna\n",
      "Aya\n",
      "Aria\n",
      "Ai\n",
      "Arika\n",
      "Amiri\n",
      "Aimi\n",
      "Ako\n",
      "Akira\n",
      "Akihi\n",
      "Ayane\n",
      "Asahi\n",
      "Ayako\n",
      "Asune\n",
      "Anna\n",
      "Arina\n",
      "Asako\n",
      "Ayano\n",
      "Asa\n",
      "Asuna\n",
      "Akana\n",
      "Ayuri\n",
      "Aiko\n",
      "Akiha\n",
      "Ayae\n",
      "Ayari\n",
      "Azusa\n",
      "Akino\n",
      "Azumi\n",
      "Amina\n",
      "Akiyo\n",
      "Aisa\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "URL = \"https://japanese-names.info/first-names/gender/girl-name/start-with-a-girl\"\n",
    "\n",
    "async def scrape_names(url):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        context = await browser.new_context(\n",
    "            user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                       \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                       \"Chrome/118.0.5993.118 Safari/537.36\"\n",
    "        )\n",
    "        page = await context.new_page()\n",
    "        await page.goto(url, timeout=0)\n",
    "\n",
    "        # Wait for at least one name to appear\n",
    "        await page.wait_for_selector(\"ul.name-list li h3\", timeout=10000)\n",
    "\n",
    "        # Scroll to load all names\n",
    "        previous_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "        while True:\n",
    "            await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            await page.wait_for_timeout(1500)  # wait for new content to load\n",
    "            current_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "            if current_height == previous_height:\n",
    "                break\n",
    "            previous_height = current_height\n",
    "\n",
    "        # Extract names with proper inner_text evaluation\n",
    "        name_elements = await page.query_selector_all(\"ul.name-list li h3\")\n",
    "        names = []\n",
    "        for el in name_elements:\n",
    "            # Use evaluate to get fully rendered text\n",
    "            text = await el.evaluate(\"(node) => node.textContent.trim()\")\n",
    "            if text:\n",
    "                names.append(text)\n",
    "\n",
    "        await browser.close()\n",
    "        return names\n",
    "\n",
    "async def main():\n",
    "    names = await scrape_names(URL)\n",
    "    print(f\"Found {len(names)} names ✅\")\n",
    "    for name in names:\n",
    "        print(name)\n",
    "\n",
    "# In Jupyter\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64874eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping girl names...\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import string\n",
    "import pandas as pd\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "BASE_URL = \"https://japanese-names.info/first-names/gender/{gender}-name/start-with-{letter}\"\n",
    "GIRL = \"girl\"\n",
    "BOY = \"boy\"\n",
    "\n",
    "async def scrape_page(page, url):\n",
    "    \"\"\"Scrape one page and return list of names\"\"\"\n",
    "    await page.goto(url, timeout=0)\n",
    "    # Wait for names to load\n",
    "    try:\n",
    "        await page.wait_for_selector(\"ul.name-list li h3\", timeout=5000)\n",
    "    except:\n",
    "        return []  # no names on this page\n",
    "\n",
    "    # Scroll to ensure dynamic content loads\n",
    "    previous_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "    while True:\n",
    "        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        await page.wait_for_timeout(1000)\n",
    "        current_height = await page.evaluate(\"document.body.scrollHeight\")\n",
    "        if current_height == previous_height:\n",
    "            break\n",
    "        previous_height = current_height\n",
    "\n",
    "    # Extract names\n",
    "    name_elements = await page.query_selector_all(\"ul.name-list li h3\")\n",
    "    names = [await el.evaluate(\"(node) => node.textContent.trim()\") for el in name_elements if await el.evaluate(\"(node) => node.textContent.trim()\")]\n",
    "    return names\n",
    "\n",
    "async def scrape_gender(gender):\n",
    "    all_data = []\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        context = await browser.new_context(\n",
    "            user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                       \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                       \"Chrome/118.0.5993.118 Safari/537.36\"\n",
    "        )\n",
    "        page = await context.new_page()\n",
    "\n",
    "        for letter in string.ascii_lowercase:\n",
    "            page_num = 1\n",
    "            while True:\n",
    "                if gender == GIRL:\n",
    "                    url = f\"{BASE_URL.format(gender=gender, letter=letter)}/page/{page_num}/\"\n",
    "                else:  # boys have no page number on single-page\n",
    "                    url = f\"{BASE_URL.format(gender=gender, letter=letter)}\" if page_num == 1 else f\"{BASE_URL.format(gender=gender, letter=letter)}/page/{page_num}/\"\n",
    "\n",
    "                names = await scrape_page(page, url)\n",
    "                if not names:\n",
    "                    break  # stop if page has no names\n",
    "\n",
    "                # Append data\n",
    "                for name in names:\n",
    "                    all_data.append({\"letter\": letter, \"page\": page_num, \"name\": name})\n",
    "\n",
    "                page_num += 1\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "async def main():\n",
    "    print(\"Scraping girl names...\")\n",
    "    girls_df = await scrape_gender(GIRL)\n",
    "    print(f\"Found {len(girls_df)} girl names ✅\")\n",
    "\n",
    "    print(\"Scraping boy names...\")\n",
    "    boys_df = await scrape_gender(BOY)\n",
    "    print(f\"Found {len(boys_df)} boy names ✅\")\n",
    "\n",
    "    # Save to CSV if needed\n",
    "    girls_df.to_csv(\"japanese_girl_names.csv\", index=False)\n",
    "    boys_df.to_csv(\"japanese_boy_names.csv\", index=False)\n",
    "\n",
    "    return girls_df, boys_df\n",
    "\n",
    "# In Jupyter, use:\n",
    "girls_df, boys_df = await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76f2adf8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mnames\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'names' is not defined"
     ]
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786d1661",
   "metadata": {},
   "outputs": [],
   "source": [
    "#names.to_parquet(\"conf_names.parquet\", index=False)\n",
    "s3_path = \"s3://lab/fuzzed.csv\"  # bucket + key\n",
    "\n",
    "# Save directly to MinIO\n",
    "df[~df[\"Score\"].isna()].to_csv(s3_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e469ebcc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     16\u001b[39m             articles.append({\n\u001b[32m     17\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m\"\u001b[39m: title,\n\u001b[32m     18\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mjournal\u001b[39m\u001b[33m\"\u001b[39m: journal,\n\u001b[32m     19\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33myear\u001b[39m\u001b[33m\"\u001b[39m: pub_date,\n\u001b[32m     20\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mauthors\u001b[39m\u001b[33m\"\u001b[39m: authors_str\n\u001b[32m     21\u001b[39m             })\n\u001b[32m     23\u001b[39m             elem.clear()\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticles\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/pandas/core/frame.py:855\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    853\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    854\u001b[39m         columns = ensure_index(columns)\n\u001b[32m--> \u001b[39m\u001b[32m855\u001b[39m     arrays, columns, index = \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[32m    857\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[32m    858\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    861\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    863\u001b[39m     mgr = arrays_to_mgr(\n\u001b[32m    864\u001b[39m         arrays,\n\u001b[32m    865\u001b[39m         columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    868\u001b[39m         typ=manager,\n\u001b[32m    869\u001b[39m     )\n\u001b[32m    870\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/pandas/core/internals/construction.py:520\u001b[39m, in \u001b[36mnested_data_to_arrays\u001b[39m\u001b[34m(data, columns, index, dtype)\u001b[39m\n\u001b[32m    517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[32m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    518\u001b[39m     columns = ensure_index(data[\u001b[32m0\u001b[39m]._fields)\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m arrays, columns = \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    521\u001b[39m columns = ensure_index(columns)\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/pandas/core/internals/construction.py:837\u001b[39m, in \u001b[36mto_arrays\u001b[39m\u001b[34m(data, columns, dtype)\u001b[39m\n\u001b[32m    835\u001b[39m     arr = _list_to_arrays(data)\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data[\u001b[32m0\u001b[39m], abc.Mapping):\n\u001b[32m--> \u001b[39m\u001b[32m837\u001b[39m     arr, columns = \u001b[43m_list_of_dict_to_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m    839\u001b[39m     arr, columns = _list_of_series_to_arrays(data, columns)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/pandas/core/internals/construction.py:922\u001b[39m, in \u001b[36m_list_of_dict_to_arrays\u001b[39m\u001b[34m(data, columns)\u001b[39m\n\u001b[32m    918\u001b[39m     columns = ensure_index(pre_cols)\n\u001b[32m    920\u001b[39m \u001b[38;5;66;03m# assure that they are of the base dict class and not of derived\u001b[39;00m\n\u001b[32m    921\u001b[39m \u001b[38;5;66;03m# classes\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m data = [d \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(d) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mdict\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]  \u001b[38;5;66;03m# noqa: E721\u001b[39;00m\n\u001b[32m    924\u001b[39m content = lib.dicts_to_array(data, \u001b[38;5;28mlist\u001b[39m(columns))\n\u001b[32m    925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "articles = []\n",
    "\n",
    "with fs.open(path, 'rb') as f:\n",
    "    with gzip.GzipFile(fileobj=f) as gz:\n",
    "        #parser = etree.XMLParser(recover=True, resolve_entities=True)\n",
    "        context = etree.iterparse(gz, events=(\"end\",), tag=\"article\", recover=True)\n",
    "        \n",
    "        for event, elem in context:\n",
    "            title = elem.findtext(\"title\")\n",
    "            journal = elem.findtext(\"journal\")\n",
    "            pub_date = elem.findtext(\"year\")\n",
    "\n",
    "            authors = [a.text for a in elem.findall(\".//author\")]\n",
    "            authors_str = \"; \".join([a for a in authors if a])\n",
    "\n",
    "            articles.append({\n",
    "                \"title\": title,\n",
    "                \"journal\": journal,\n",
    "                \"year\": pub_date,\n",
    "                \"authors\": authors_str\n",
    "            })\n",
    "\n",
    "            elem.clear()\n",
    "\n",
    "df = pd.DataFrame(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f95077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpro = []\n",
    "\n",
    "with fs.open(path, 'rb') as f:\n",
    "    with gzip.GzipFile(fileobj=f) as gz:\n",
    "        #parser = etree.XMLParser(recover=True, resolve_entities=True)\n",
    "        context = etree.iterparse(gz, events=(\"end\",), tag=\"inproceedings\", recover=True)\n",
    "        \n",
    "        for event, elem in context:\n",
    "            title = elem.findtext(\"title\")\n",
    "            conf = elem.findtext(\"booktitle\")\n",
    "            pub_date = elem.findtext(\"year\")\n",
    "\n",
    "            authors = [a.text for a in elem.findall(\".//author\")]\n",
    "            authors_str = \"; \".join([a for a in authors if a])\n",
    "\n",
    "            inpro.append({\n",
    "                \"title\": title,\n",
    "                \"conf\": conf,\n",
    "                \"year\": pub_date,\n",
    "                \"authors\": authors_str\n",
    "            })\n",
    "\n",
    "            elem.clear()\n",
    "\n",
    "df = pd.DataFrame(inpro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aafcd72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>conf</th>\n",
       "      <th>year</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Future of Classic Data Administration: Obj...</td>\n",
       "      <td>SWEE</td>\n",
       "      <td>1998</td>\n",
       "      <td>Arnon Rosenthal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Some Patterns of Convincing Software Engineeri...</td>\n",
       "      <td>Denert Award</td>\n",
       "      <td>2020</td>\n",
       "      <td>Lutz Prechelt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Static Worst-Case Analyses and Their Validatio...</td>\n",
       "      <td>Denert Award</td>\n",
       "      <td>2020</td>\n",
       "      <td>Peter Wgemann</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crossing Disciplinary Borders to Improve Requi...</td>\n",
       "      <td>Denert Award</td>\n",
       "      <td>2020</td>\n",
       "      <td>Anne Hess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What You See Is What You Get: Practical Effect...</td>\n",
       "      <td>Denert Award</td>\n",
       "      <td>2020</td>\n",
       "      <td>Jonathan Immanuel Brachthuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749504</th>\n",
       "      <td>Machine Intentionality, the Moral Status of Ma...</td>\n",
       "      <td>PT-AI</td>\n",
       "      <td>2011</td>\n",
       "      <td>David Leech Anderson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749509</th>\n",
       "      <td>Practical Introspection as Inspiration for AI.</td>\n",
       "      <td>PT-AI</td>\n",
       "      <td>2011</td>\n",
       "      <td>Sam Freed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749510</th>\n",
       "      <td>\"Computational Ontology and Deontology\".</td>\n",
       "      <td>PT-AI</td>\n",
       "      <td>2011</td>\n",
       "      <td>Raffaela Giovagnoli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749512</th>\n",
       "      <td>Emotional Control-Conditio Sine Qua Non for Ad...</td>\n",
       "      <td>PT-AI</td>\n",
       "      <td>2011</td>\n",
       "      <td>Claudius Gros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749513</th>\n",
       "      <td>Becoming Digital: Reconciling Theories of Digi...</td>\n",
       "      <td>PT-AI</td>\n",
       "      <td>2011</td>\n",
       "      <td>Harry Halpin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284765 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     title          conf  \\\n",
       "0        The Future of Classic Data Administration: Obj...          SWEE   \n",
       "1        Some Patterns of Convincing Software Engineeri...  Denert Award   \n",
       "2        Static Worst-Case Analyses and Their Validatio...  Denert Award   \n",
       "3        Crossing Disciplinary Borders to Improve Requi...  Denert Award   \n",
       "4        What You See Is What You Get: Practical Effect...  Denert Award   \n",
       "...                                                    ...           ...   \n",
       "3749504  Machine Intentionality, the Moral Status of Ma...         PT-AI   \n",
       "3749509     Practical Introspection as Inspiration for AI.         PT-AI   \n",
       "3749510           \"Computational Ontology and Deontology\".         PT-AI   \n",
       "3749512  Emotional Control-Conditio Sine Qua Non for Ad...         PT-AI   \n",
       "3749513  Becoming Digital: Reconciling Theories of Digi...         PT-AI   \n",
       "\n",
       "         year                        authors  \n",
       "0        1998                Arnon Rosenthal  \n",
       "1        2020                  Lutz Prechelt  \n",
       "2        2020                  Peter Wgemann  \n",
       "3        2020                      Anne Hess  \n",
       "4        2020  Jonathan Immanuel Brachthuser  \n",
       "...       ...                            ...  \n",
       "3749504  2011           David Leech Anderson  \n",
       "3749509  2011                      Sam Freed  \n",
       "3749510  2011            Raffaela Giovagnoli  \n",
       "3749512  2011                  Claudius Gros  \n",
       "3749513  2011                   Harry Halpin  \n",
       "\n",
       "[284765 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "df[df[\"authors\"].apply(lambda x: bool(re.compile(r\"^[A-Za-z\\s-]+$\").search(str(x))))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18043c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['author', 'booktitle', 'cdrom', 'cite', 'crossref', 'editor', 'ee', 'month', 'note', 'number', 'pages', 'publnr', 'stream', 'title', 'url', 'volume', 'year']\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "import gzip\n",
    "\n",
    "direct_set = set()\n",
    "\n",
    "with fs.open(path, \"rb\") as f:\n",
    "    with gzip.GzipFile(fileobj=f) as gz:\n",
    "        context = etree.iterparse(gz, events=(\"end\",), tag=\"inproceedings\", recover=True)\n",
    "        for event, elem in context:\n",
    "            # collect child tags\n",
    "            for child in elem:\n",
    "                if child.tag is not None:\n",
    "                    direct_set.add(child.tag)\n",
    "\n",
    "            # clear memory: remove element and its previous siblings\n",
    "            elem.clear()\n",
    "            while elem.getprevious() is not None:\n",
    "                del elem.getparent()[0]\n",
    "\n",
    "print(sorted(direct_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f63b4a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "n_chunks = 50\n",
    "chunk_size = len(df) // n_chunks\n",
    "\n",
    "for i in range(n_chunks):\n",
    "    start = i * chunk_size\n",
    "    end = (i + 1) * chunk_size if i < n_chunks - 1 else len(df)  # last chunk includes remainder\n",
    "    \n",
    "    chunk = df.iloc[start:end].copy()\n",
    "    \n",
    "    # Split authors column into multiple columns\n",
    "    #authors_df = chunk['authors'].str.split(';', expand=True)\n",
    "    #authors_df.columns = [f'author{i+1}' for i in range(authors_df.shape[1])]\n",
    "    \n",
    "    # Combine with original chunk\n",
    "    #chunk = pd.concat([chunk.drop(columns=['authors']), authors_df], axis=1)\n",
    "    \n",
    "    chunk.to_parquet(f\"inpro/inproceedings_chunk_{i}.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a780155",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "\n",
    "ego_data = defaultdict(lambda: {\"publications\": []}) #set permet de créer des clefs si absentes et de les actualiser si présentes (en définissant la classe des clefs)\n",
    "\n",
    "chunk_files = glob.glob(\"art/articles_chunk_*.parquet\")\n",
    "\n",
    "for file in chunk_files:\n",
    "    df_chunk = pd.read_parquet(file)\n",
    "    for _, row in df_chunk.iterrows():\n",
    "        authors = row['authors'].split(';')\n",
    "        authors = [a for a in authors if not re.compile(r\"\\d\").search(a)]\n",
    "        for ego in authors:\n",
    "            coauthors = set(authors) - {ego}\n",
    "            \n",
    "            ego_data[ego][\"publications\"].append({\n",
    "                \"coauthors\": list(coauthors),\n",
    "                \"journal\": row.get(\"journal\"),\n",
    "                \"year\": row.get(\"year\"),\n",
    "                \"title\": row.get(\"title\")\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f903d5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ego_df = pd.DataFrame([\n",
    "    {'ego': ego, **data[\"publications\"]} #récup paries ego - alter et trie dans l'ordre alphabétique\n",
    "    for ego, data in ego_data.items()\n",
    "])\"\"\"\n",
    "rows = []\n",
    "for ego, data in ego_data.items():\n",
    "    for pub in data[\"publications\"]:\n",
    "        rows.append({\n",
    "            \"ego\": ego,\n",
    "            \"coauthors\": \",\".join(sorted(pub[\"coauthors\"])),\n",
    "            \"journal\": pub.get(\"journal\"),\n",
    "            \"year\": pub.get(\"year\"),\n",
    "            \"title\": pub.get(\"title\")\n",
    "        })\n",
    "\n",
    "ego_df = pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7e84aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_df[\"name\"]=ego_df[\"ego\"].str.split(n=1).str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99995cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "names=ego_df[\"name\"].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5bdac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "names=names.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "810db5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#names.to_parquet(\"conf_names.parquet\", index=False)\n",
    "s3_path = \"s3://lab/art_names.csv\"  # bucket + key\n",
    "\n",
    "# Save directly to MinIO\n",
    "names.to_csv(s3_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df5bc00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_df.to_parquet(\"ego_df.parquet\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "779e4206",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fs.open(\"s3://lab/name_gender_dataset.csv\") as f:\n",
    "    g1 = pd.read_csv(f)\n",
    "with fs.open(\"s3://lab/wgnd_2_0_name-gender-langcode.csv\") as f:\n",
    "    g2 = pd.read_csv(f)\n",
    "\n",
    "g1=g1[[\"Name\",\"Gender\"]]\n",
    "g2[[\"Name\",\"Gender\"]]=g2[[\"name\",\"gender\"]]\n",
    "g2=g2[[\"Name\",\"Gender\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4106024c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>a laura</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>a lelia</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>a lelia</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>a lerah</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>a lerah</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>a lex ander</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>a lex ander</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>a lie</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>a lisa</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>a lisa</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>a liza</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>a liza</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>a lmos</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>a luan</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>a mang</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>a me</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>a men</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>a mer</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>a mer</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>a merie</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>a merie</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>a mou</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>a nai</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>a nak la</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>a nak la</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>a nao</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>a neng</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>a nette</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>a niang</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>a niyah</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>a niyah</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>a nv</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>a ou</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>a pan</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>a pao</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>a pi</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>a pu</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>a qie</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>a qiong</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>a que</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>a ran</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>a rang</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>a rao</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>a re</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>a ri</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>a rin</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>a rin</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>a ron</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>a ron</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>a s m</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Name Gender\n",
       "100      a laura      F\n",
       "101      a lelia      F\n",
       "102      a lelia      F\n",
       "103      a lerah      F\n",
       "104      a lerah      F\n",
       "105  a lex ander      M\n",
       "106  a lex ander      M\n",
       "107        a lie      M\n",
       "108       a lisa      F\n",
       "109       a lisa      F\n",
       "110       a liza      F\n",
       "111       a liza      F\n",
       "112       a lmos      M\n",
       "113       a luan      M\n",
       "114       a mang      F\n",
       "115         a me      F\n",
       "116        a men      M\n",
       "117        a mer      M\n",
       "118        a mer      M\n",
       "119      a merie      F\n",
       "120      a merie      F\n",
       "121        a mou      M\n",
       "122        a nai      F\n",
       "123     a nak la      M\n",
       "124     a nak la      M\n",
       "125        a nao      F\n",
       "126       a neng      M\n",
       "127      a nette      F\n",
       "128      a niang      F\n",
       "129      a niyah      F\n",
       "130      a niyah      F\n",
       "131         a nv      F\n",
       "132         a ou      M\n",
       "133        a pan      F\n",
       "134        a pao      M\n",
       "135         a pi      M\n",
       "136         a pu      F\n",
       "137        a qie      M\n",
       "138      a qiong      F\n",
       "139        a que      M\n",
       "140        a ran      M\n",
       "141       a rang      M\n",
       "142        a rao      F\n",
       "143         a re      F\n",
       "144         a ri      M\n",
       "145        a rin      F\n",
       "146        a rin      F\n",
       "147        a ron      M\n",
       "148        a ron      M\n",
       "149        a s m      M"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g2[100:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "add36b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>James</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>John</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Robert</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Michael</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>William</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21978307</th>\n",
       "      <td>히카리</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21978308</th>\n",
       "      <td>히토미</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21978309</th>\n",
       "      <td>힘찬</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21978310</th>\n",
       "      <td>凉峰</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21978311</th>\n",
       "      <td>凉翼</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21978312 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Name Gender\n",
       "0           James      M\n",
       "1            John      M\n",
       "2          Robert      M\n",
       "3         Michael      M\n",
       "4         William      M\n",
       "...           ...    ...\n",
       "21978307      히카리      F\n",
       "21978308      히토미      F\n",
       "21978309       힘찬      M\n",
       "21978310       凉峰      M\n",
       "21978311       凉翼      M\n",
       "\n",
       "[21978312 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([g1,g2],axis=0,ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
